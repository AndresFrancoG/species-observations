{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Add the parameters tag to this cell\n",
    "kedro_env = 'local'\n",
    "# kedro_env = 'test_cloud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_project_path = '..//'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from kedro_datasets.pandas import GBQTableDataSet, CSVDataSet\n",
    "from kedro.io import PartitionedDataSet\n",
    "from typing import Dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads kedro yml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.config import ConfigLoader\n",
    "from kedro.framework.project import settings\n",
    "\n",
    "KEDRO_ENV = \"local\"\n",
    "conf_path = str(\"../\" + settings.CONF_SOURCE)\n",
    "conf_loader = ConfigLoader(conf_source=conf_path, env=kedro_env)\n",
    "\n",
    "conf_catalog = conf_loader[\"catalog\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query using bigquery API for query debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bq_query_cat_entry = catalog['bq_test']\n",
    "# project_id = bq_query_cat_entry['project']\n",
    "\n",
    "# Recovers contents of ARRAYS nested inside STRUCTS\n",
    "# sql = \"\"\"SELECT element.array_element\n",
    "#     FROM bigquery-public-data.gbif.occurrences,\n",
    "#     UNNEST(issue.array) AS element\n",
    "#     LIMIT 20\"\"\"\n",
    "\n",
    "# Shows the distinct values of the issue column, which is an ARRAY nested inside a STRUCT\n",
    "# sql = \"\"\"SELECT distinct element.array_element\n",
    "#     FROM bigquery-public-data.gbif.occurrences,\n",
    "#     UNNEST(issue.array) AS element\"\"\"\n",
    "\n",
    "# Counts the number of observations per species and orders it from high to low\n",
    "# sql = \"\"\" SELECT species,\n",
    "#   count(species) as cnt\n",
    "#   FROM bigquery-public-data.gbif.occurrences\n",
    "#   group by species\n",
    "#   order by cnt desc\"\"\"\n",
    "\n",
    "# Counts the number of observations per species and orders it from high to low\n",
    "# sql = \"\"\" SELECT species,\n",
    "#   count(species) as cnt\n",
    "#   FROM bigquery-public-data.gbif.occurrences\n",
    "#   group by species\n",
    "#   order by cnt desc\"\"\"\n",
    "\n",
    "# Gets top 20 rows of all data\n",
    "# sql = \"\"\" SELECT gbifid\n",
    "#     ,datasetkey\n",
    "#     ,occurrenceid\n",
    "#     ,kingdom\n",
    "#     ,phylum\n",
    "#     ,class\n",
    "#     ,`order`\n",
    "#     ,family\n",
    "#     ,genus\n",
    "#     ,species\n",
    "#     ,infraspecificepithet\n",
    "#     ,taxonrank\n",
    "#     ,scientificname\n",
    "#     ,verbatimscientificname\n",
    "#     ,verbatimscientificnameauthorship\n",
    "#     ,countrycode\n",
    "#     ,locality\n",
    "#     ,stateprovince\n",
    "#     ,occurrencestatus\n",
    "#     ,individualcount\n",
    "#     ,publishingorgkey\n",
    "#     ,decimallatitude\n",
    "#     ,decimallongitude\n",
    "#     ,coordinateuncertaintyinmeters\n",
    "#     ,coordinateprecision\n",
    "#     ,elevation\n",
    "#     ,elevationaccuracy\n",
    "#     ,depth\n",
    "#     ,depthaccuracy\n",
    "#     ,eventdate\n",
    "#     ,day\n",
    "#     ,month\n",
    "#     ,year\n",
    "#     ,taxonkey\n",
    "#     ,specieskey\n",
    "#     ,basisofrecord\n",
    "#     ,institutioncode\n",
    "#     ,collectioncode\n",
    "#     ,catalognumber\n",
    "#     ,recordnumber\n",
    "#     ,identifiedby\n",
    "#     ,dateidentified\n",
    "#     ,license\n",
    "#     ,rightsholder\n",
    "#     ,recordedby\n",
    "#     ,typestatus\n",
    "#     ,establishmentmeans\n",
    "#     ,lastinterpreted\n",
    "#     ,mediatype\n",
    "#     ,issue\n",
    "# FROM bigquery-public-data.gbif.occurrences\n",
    "# LIMIT 20 \"\"\"\n",
    "\n",
    "# \n",
    "# sql = \"\"\"SELECT gbifid\n",
    "#     ,kingdom\n",
    "#     ,phylum\n",
    "#     ,class\n",
    "#     ,`order`\n",
    "#     ,family\n",
    "#     ,genus\n",
    "#     ,species\n",
    "#     ,countrycode\n",
    "#     ,occurrencestatus\n",
    "#     ,individualcount\n",
    "#     ,decimallatitude\n",
    "#     ,decimallongitude\n",
    "#     ,eventdate\n",
    "#     ,day\n",
    "#     ,month\n",
    "#     ,year\n",
    "#     ,basisofrecord \n",
    "#     ,element.array_element as issue\n",
    "#     FROM bigquery-public-data.gbif.occurrences\n",
    "#     CROSS JOIN UNNEST(issue.array) AS element\n",
    "#     WHERE species = 'Anas platyrhynchos' AND eventdate > '2020-01-01'\n",
    "#     \"\"\"\n",
    "\n",
    "# data_set = GBQQueryDataSet(sql, project=project_id)\n",
    "# df = data_set.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads csv with data downladed from bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_entry = 'species_data'\n",
    "path = relative_project_path + conf_catalog[catalog_entry]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kedro_partitionedDS(path: str, dataset: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Loads as a single dataframe the partitioned data stored in path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the partitioned data\n",
    "    dataset : Dict\n",
    "        Type of data to search for and load options (dataset option in kedro's catalog of type PartitionedDataSet)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        joined dataset\n",
    "    \"\"\"\n",
    "    data_set = PartitionedDataSet(\n",
    "        path=path,\n",
    "        dataset=dataset,\n",
    "    )\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    loaded = data_set.load()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for partition_id, partition_load_func in loaded.items():\n",
    "        partition_data = partition_load_func()\n",
    "        df = pd.concat(\n",
    "            [df, partition_data], ignore_index=True, sort=True\n",
    "        )\n",
    "    return df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kedro_env == 'local':\n",
    "    dataset = conf_catalog[catalog_entry]['dataset']\n",
    "    df = load_kedro_partitionedDS(path, dataset)\n",
    "elif kedro_env == 'test_cloud':\n",
    "    data_set = CSVDataSet(filepath=path)\n",
    "    df = data_set.load()\n",
    "else:\n",
    "    raise ValueError('Undefined kedro environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generates sample data for automatic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "# test_catalog_entry = 'species_data'\n",
    "# test_path = '..//' + conf_catalog[test_catalog_entry]['path']\n",
    "\n",
    "# sample_index = combine_all.species.sample(1000).index\n",
    "# sample_df = combine_all.loc[sample_index]\n",
    "# sample_df.to_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups up data by observations per day/month and plots the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cat = 'M'  #Choose either 'D' for day or 'M' for month\n",
    "count_col = 'individualcount'\n",
    "date_col = 'eventdate'\n",
    "date_col_datetime = date_col + '_datetime'\n",
    "\n",
    "df[date_col_datetime] = pd.to_datetime( df[date_col].str[:-4], format='%Y-%m-%d %H:%M:%S')\n",
    "df[count_col] = df[count_col].fillna(0)\n",
    "df_out = df[[count_col, date_col_datetime]]\n",
    "df_out = df_out.set_index(date_col_datetime).resample(group_cat).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Observations per '\n",
    "if group_cat == 'D':\n",
    "    title = title + 'day'\n",
    "elif group_cat == 'M':\n",
    "    title = title + 'month'    \n",
    "df_out.plot(title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "species",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
